{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part One: Create Simpler Dataframe for Holding Articles</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>source_num</th>\n",
       "      <th>regexed</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_len</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China urges greater efforts to ease Korean ten...</td>\n",
       "      <td>Global Times (China)</td>\n",
       "      <td>China hopes all parties would do more to ease ...</td>\n",
       "      <td>0</td>\n",
       "      <td>china hopes all parties would do more to ease ...</td>\n",
       "      <td>['china', 'hopes', 'parties', 'would', 'ease',...</td>\n",
       "      <td>276</td>\n",
       "      <td>51</td>\n",
       "      <td>['china', 'hope', 'party', 'would', 'ease', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multilateral cooperation involving China best ...</td>\n",
       "      <td>Global Times (China)</td>\n",
       "      <td>The US and several Western powers including Au...</td>\n",
       "      <td>0</td>\n",
       "      <td>the us and several western powers including au...</td>\n",
       "      <td>['us', 'several', 'western', 'powers', 'includ...</td>\n",
       "      <td>273</td>\n",
       "      <td>79</td>\n",
       "      <td>['u', 'several', 'western', 'power', 'includin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despite pandemic at home, US continues to make...</td>\n",
       "      <td>Global Times (China)</td>\n",
       "      <td>Illustration: Liu Rui/GT\\n\\n\\n\\nOver the last ...</td>\n",
       "      <td>0</td>\n",
       "      <td>illustration  liu rui gt    over the last two ...</td>\n",
       "      <td>['illustration', 'liu', 'rui', 'gt', 'last', '...</td>\n",
       "      <td>377</td>\n",
       "      <td>67</td>\n",
       "      <td>['illustration', 'liu', 'rui', 'gt', 'last', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cut Chinese firms in eye of the storm a break:...</td>\n",
       "      <td>Global Times (China)</td>\n",
       "      <td>Photo: IC\\n\\n\\n\\n\\nAccording to Reuters, Chine...</td>\n",
       "      <td>0</td>\n",
       "      <td>photo  ic     according to reuters  chinese le...</td>\n",
       "      <td>['photo', 'ic', 'according', 'reuters', 'chine...</td>\n",
       "      <td>438</td>\n",
       "      <td>69</td>\n",
       "      <td>['photo', 'ic', 'according', 'reuters', 'chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finnair launches new flight services for China</td>\n",
       "      <td>Global Times (China)</td>\n",
       "      <td>Lars Olofsson, Greater China sales director at...</td>\n",
       "      <td>0</td>\n",
       "      <td>lars olofsson  greater china sales director at...</td>\n",
       "      <td>['lars', 'olofsson', 'greater', 'china', 'sale...</td>\n",
       "      <td>361</td>\n",
       "      <td>46</td>\n",
       "      <td>['lars', 'olofsson', 'greater', 'china', 'sale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                source  \\\n",
       "0  China urges greater efforts to ease Korean ten...  Global Times (China)   \n",
       "1  Multilateral cooperation involving China best ...  Global Times (China)   \n",
       "2  Despite pandemic at home, US continues to make...  Global Times (China)   \n",
       "3  Cut Chinese firms in eye of the storm a break:...  Global Times (China)   \n",
       "4     Finnair launches new flight services for China  Global Times (China)   \n",
       "\n",
       "                                             content  source_num  \\\n",
       "0  China hopes all parties would do more to ease ...           0   \n",
       "1  The US and several Western powers including Au...           0   \n",
       "2  Illustration: Liu Rui/GT\\n\\n\\n\\nOver the last ...           0   \n",
       "3  Photo: IC\\n\\n\\n\\n\\nAccording to Reuters, Chine...           0   \n",
       "4  Lars Olofsson, Greater China sales director at...           0   \n",
       "\n",
       "                                             regexed  \\\n",
       "0  china hopes all parties would do more to ease ...   \n",
       "1  the us and several western powers including au...   \n",
       "2  illustration  liu rui gt    over the last two ...   \n",
       "3  photo  ic     according to reuters  chinese le...   \n",
       "4  lars olofsson  greater china sales director at...   \n",
       "\n",
       "                                              tokens  word_count  title_len  \\\n",
       "0  ['china', 'hopes', 'parties', 'would', 'ease',...         276         51   \n",
       "1  ['us', 'several', 'western', 'powers', 'includ...         273         79   \n",
       "2  ['illustration', 'liu', 'rui', 'gt', 'last', '...         377         67   \n",
       "3  ['photo', 'ic', 'according', 'reuters', 'chine...         438         69   \n",
       "4  ['lars', 'olofsson', 'greater', 'china', 'sale...         361         46   \n",
       "\n",
       "                                              lemmas  \n",
       "0  ['china', 'hope', 'party', 'would', 'ease', 't...  \n",
       "1  ['u', 'several', 'western', 'power', 'includin...  \n",
       "2  ['illustration', 'liu', 'rui', 'gt', 'last', '...  \n",
       "3  ['photo', 'ic', 'according', 'reuters', 'chine...  \n",
       "4  ['lars', 'olofsson', 'greater', 'china', 'sale...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('china_articles_cleaned.csv')\n",
    "articles.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.drop(columns=['regexed', 'tokens', 'word_count', 'title_len', 'lemmas'], inplace=True)\n",
    "\n",
    "articles.to_csv('articles_for_finetuning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part Two: Create .txt Files for Content of Each Source</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df = pd.read_csv('gpt3_outputs.csv')\n",
    "articles_df = pd.read_csv('articles_for_finetuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrm311/opt/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# We don't want to fine-tune on the content of articles where we will then use the corresponding headline to generate an output\n",
    "\n",
    "# To prevent this, first make a list of all the headlines we intend to use later\n",
    "titles = outputs_df['title'].values\n",
    "\n",
    "# Then create a to_use field in the df of articles\n",
    "articles_df['to_use'] = True\n",
    "\n",
    "# And set it to False if the title matches one of the headlines in our list\n",
    "for title in titles:\n",
    "    articles_df['to_use'].loc[articles_df[articles_df['title']==title].index.values] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     15000\n",
       "unique        2\n",
       "top        True\n",
       "freq      14875\n",
       "Name: to_use, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's confirm that the previous stage excluded 125 articles\n",
    "articles_df['to_use'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_file(source_id):\n",
    "    lines_list = []\n",
    "    to_write_df = articles_df[(articles_df['to_use']==True) & (articles_df['source_num']==source_id)]\n",
    "    for id in to_write_df.index.values:\n",
    "        lines_list.append(to_write_df['title'].loc[id] + \"::\" + to_write_df['content'].loc[id])\n",
    "        \n",
    "    f = open('source_'+str(source_id)+'.txt','w')\n",
    "    f.writelines(lines_list)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    create_txt_file(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part Three: Create Fine-Tuned GPT-2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init LanguageModels\n",
    "!git clone https://www.github.com/huggingface/transformers\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I know 5 copy-pastes where the only change is the numbers 0-5 at the end of file name is dumb but\n",
    "#it may be easier to read for most than bash scripts from a web notebook.\n",
    "!python transformers/examples/language-modeling/run_clm.py --model_name_or_path gpt2 --train_file 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/source_0.txt' --do_train --output_dir 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/Tuned-Model_0' --per_device_train_batch_size=1\n",
    "!python transformers/examples/language-modeling/run_clm.py --model_name_or_path gpt2 --train_file 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/source_1.txt' --do_train --output_dir 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/Tuned-Model_1' --per_device_train_batch_size=1\n",
    "!python transformers/examples/language-modeling/run_clm.py --model_name_or_path gpt2 --train_file 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/source_2.txt' --do_train --output_dir 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/Tuned-Model_2' --per_device_train_batch_size=1\n",
    "!python transformers/examples/language-modeling/run_clm.py --model_name_or_path gpt2 --train_file 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/source_3.txt' --do_train --output_dir 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/Tuned-Model_3' --per_device_train_batch_size=1\n",
    "!python transformers/examples/language-modeling/run_clm.py --model_name_or_path gpt2 --train_file 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/source_4.txt' --do_train --output_dir 'drive/Shared drives/CyberAI/Ongoing Projects/OpenAI/Proof of Concept Outputs/FineTuning/Tuned-Model_4' --per_device_train_batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part Four: Generate News Stories Using Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = path+'Tuned-Model_0'\n",
    "model_1 = path+'Tuned-Model_1'\n",
    "model_2 = path+'Tuned-Model_2'\n",
    "model_3 = path+'Tuned-Model_3'\n",
    "model_4 = path+'Tuned-Model_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_file = path+'untuned_generations.txt'\n",
    "model_0_file = path+'tuned_0_generations.txt'\n",
    "model_1_file = path+'tuned_1_generations.txt'\n",
    "model_2_file = path+'tuned_2_generations.txt'\n",
    "model_3_file = path+'tuned_3_generations.txt'\n",
    "model_4_file = path+'tuned_4_generations.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each headline write a story by using each model and add it to a file of stories for that model. \n",
    "for id in outputs_df.index.values:\n",
    "  headline = outputs_df['title'].loc[id]\n",
    "  headline_for_untuned = headline+'\\n'\n",
    "  headline_for_tuned = headline+'::'\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path gpt2 --prompt \"$headline_for_untuned\" >> \"$untuned_file\"\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path \"$model_0\" --prompt \"$headline_for_untuned\" >> \"$model_0_file\"\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path \"$model_1\" --prompt \"$headline_for_tuned\" >> \"$model_1_file\"\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path \"$model_2\" --prompt \"$headline_for_tuned\" >> \"$model_2_file\"\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path \"$model_3\" --prompt \"$headline_for_tuned\" >> \"$model_3_file\"\n",
    "  !python transformers/examples/text-generation/run_generation.py --length 400 --model_type gpt2 --model_name_or_path \"$model_4\" --prompt \"$headline_for_tuned\" >> \"$model_4_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part Five: Put the news stories in a dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_file(filepath):\n",
    "  articles_list = []\n",
    "  f = open(filepath,'r')\n",
    "  separation_text = f.readline()\n",
    "  article_text = ''\n",
    "  for row in f:\n",
    "    if row == separation_text: #'=== GENERATED SEQUENCE 1 ===\\n':\n",
    "      articles_list.append(article_text)\n",
    "      article_text = ''\n",
    "    else:\n",
    "      article_text += row\n",
    "  articles_list.append(article_text)\n",
    "  f.close()\n",
    "  return articles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df['untuned_gpt2'] = get_articles_from_file(untuned_file)\n",
    "outputs_df['ft_source_0'] = get_articles_from_file(model_0_file)\n",
    "outputs_df['ft_source_1'] = get_articles_from_file(model_1_file)\n",
    "outputs_df['ft_source_2'] = get_articles_from_file(model_2_file)\n",
    "outputs_df['ft_source_3'] = get_articles_from_file(model_3_file)\n",
    "outputs_df['ft_source_4'] = get_articles_from_file(model_4_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df['untuned_gpt2'] = outputs_df.apply(lambda row: row['untuned_gpt2'].lstrip(row['title']), axis=1)\n",
    "outputs_df['untuned_gpt2'] = outputs_df.apply(lambda row: row['untuned_gpt2'].lstrip(\"\\n\"), axis=1)\n",
    "\n",
    "outputs_df['ft_source_0'] = outputs_df.apply(lambda row: row['ft_source_0'].lstrip(row['title']), axis=1)\n",
    "outputs_df['ft_source_0'] = outputs_df.apply(lambda row: row['ft_source_0'].lstrip(\"\\n\"), axis=1)\n",
    "outputs_df['ft_source_0'] = outputs_df.apply(lambda row: row['ft_source_0'].lstrip(\"::\"), axis=1)\n",
    "\n",
    "outputs_df['ft_source_1'] = outputs_df.apply(lambda row: row['ft_source_1'].lstrip(row['title']), axis=1)\n",
    "outputs_df['ft_source_1'] = outputs_df.apply(lambda row: row['ft_source_1'].lstrip(\"\\n\"), axis=1)\n",
    "outputs_df['ft_source_1'] = outputs_df.apply(lambda row: row['ft_source_1'].lstrip(\"::\"), axis=1)\n",
    "\n",
    "outputs_df['ft_source_2'] = outputs_df.apply(lambda row: row['ft_source_2'].lstrip(row['title']), axis=1)\n",
    "outputs_df['ft_source_2'] = outputs_df.apply(lambda row: row['ft_source_2'].lstrip(\"\\n\"), axis=1)\n",
    "outputs_df['ft_source_2'] = outputs_df.apply(lambda row: row['ft_source_2'].lstrip(\"::\"), axis=1)\n",
    "\n",
    "outputs_df['ft_source_3'] = outputs_df.apply(lambda row: row['ft_source_3'].lstrip(row['title']), axis=1)\n",
    "outputs_df['ft_source_3'] = outputs_df.apply(lambda row: row['ft_source_3'].lstrip(\"\\n\"), axis=1)\n",
    "outputs_df['ft_source_3'] = outputs_df.apply(lambda row: row['ft_source_3'].lstrip(\"::\"), axis=1)\n",
    "\n",
    "outputs_df['ft_source_4'] = outputs_df.apply(lambda row: row['ft_source_4'].lstrip(row['title']), axis=1)\n",
    "outputs_df['ft_source_4'] = outputs_df.apply(lambda row: row['ft_source_4'].lstrip(\"\\n\"), axis=1)\n",
    "outputs_df['ft_source_4'] = outputs_df.apply(lambda row: row['ft_source_4'].lstrip(\"::\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_df.to_csv(path+'outputs_with_gpt2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
